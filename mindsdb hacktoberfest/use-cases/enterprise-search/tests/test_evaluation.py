#!/usr/bin/env python3
"""
Complete Knowledge Base Evaluation with Test Table Creation.

This script:
1. Creates test tables for all Knowledge Bases
2. Evaluates all KBs using the official EVALUATE command
3. Generates comprehensive metrics (MRR, Hit@k, relevancy)
4. Creates visual charts
5. Displays formatted report
"""

import os
import sys

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from utils.create_test_tables import TestTableCreator
from utils.evaluate_kb import KBEvaluator


def main():
    """Run complete evaluation with test table creation."""
    print("ğŸš€ Complete Knowledge Base Evaluation")
    print("=" * 70)
    print("This demo will:")
    print("  â€¢ Create test tables for all Knowledge Bases")
    print("  â€¢ Evaluate using MindsDB's EVALUATE command")
    print("  â€¢ Calculate metrics: MRR, Hit@k, relevancy scores")
    print("  â€¢ Generate visual charts")
    print("  â€¢ Display comprehensive report")
    print("=" * 70)

    # Step 1: Create test tables
    print("\nğŸ”§ Step 1: Creating Test Tables...")
    print("-" * 70)
    table_creator = TestTableCreator()
    test_tables = table_creator.create_all_test_tables()

    # Verify test tables
    print("\nğŸ” Verifying test tables...")
    print("-" * 70)
    valid_tables = {}
    for kb_name, table_name in test_tables.items():
        if table_name and table_creator.verify_test_table(table_name):
            valid_tables[kb_name] = table_name
            print(f"âœ… {kb_name}: {table_name} is ready")
        else:
            print(f"âš ï¸  {kb_name}: Test table not available, will use fallback")

    # Step 2: Evaluate all Knowledge Bases
    print("\nğŸ“Š Step 2: Evaluating Knowledge Bases...")
    print("-" * 70)
    evaluator = KBEvaluator()
    metrics = evaluator.evaluate_all_kbs(test_tables=valid_tables)

    # Step 3: Generate report
    print("\nğŸ“‹ Step 3: Generating Report...")
    print("-" * 70)
    report = evaluator.create_evaluation_report(metrics)
    print(report)

    # Step 4: Create visualizations
    print("\nğŸ“Š Step 4: Creating Visual Charts...")
    print("-" * 70)
    evaluator.visualize_metrics(metrics)

    # Step 5: Summary
    print("\n" + "=" * 70)
    print("âœ… COMPLETE EVALUATION FINISHED")
    print("=" * 70)
    print("\nğŸ“Š Generated Files:")
    print("  â€¢ evaluation_charts/kb_evaluation_comparison.png")
    print("  â€¢ evaluation_charts/kb_detailed_metrics.png")
    print("\nğŸ’¡ What was evaluated:")
    print("  â€¢ Test tables created from actual KB documents")
    print("  â€¢ Official MindsDB EVALUATE command used")
    print("  â€¢ Metrics include MRR, Hit@k, and recall scores")
    print("  â€¢ Visual charts show cross-KB comparison")
    print("\nğŸ¯ Next Steps:")
    print("  â€¢ Review the metrics and charts")
    print("  â€¢ Include in your competition submission")
    print("  â€¢ Share results with your team")
    print("\nğŸ“‹ Test Tables Created:")
    for kb_name, table_name in valid_tables.items():
        if table_name:
            print(f"   â€¢ {kb_name}: {table_name}")


if __name__ == "__main__":
    main()
